{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import google.generativeai as genai\n",
    "import typing_extensions as typing\n",
    "import json\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Create the model\n",
    "quiz_generation_config = {\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 20,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "summary_generation_config = {\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 60,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "GEMINI_PRO_2_5 = \"gemini-2.5-flash-preview-05-20\"\n",
    "\n",
    "quiz_model = genai.GenerativeModel(\n",
    "  model_name=GEMINI_PRO_2_5,\n",
    "  generation_config=quiz_generation_config,\n",
    "  system_instruction=\"You are a helpful assistant which helps teachers generate quiz from given content based on user requirements.\",\n",
    ")\n",
    "summary_model = genai.GenerativeModel(\n",
    "  model_name=GEMINI_PRO_2_5,\n",
    "  generation_config=summary_generation_config,\n",
    "  system_instruction=\"You are a helpful assistant which helps teachers generate summary from given content based on user requirements.\",\n",
    ")\n",
    "\n",
    "quiz_chat_session = quiz_model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "summary_chat_session = summary_model.start_chat(\n",
    "  history=[\n",
    "  ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# bring in deps\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "\n",
    "# use SimpleDirectoryReader to parse our file\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# Ensure the async method is awaited\n",
    "import asyncio\n",
    "\n",
    "# Define a global variable to store the text\n",
    "\n",
    "async def load_documents():\n",
    "    global all_text\n",
    "    global documents\n",
    "    documents = await SimpleDirectoryReader(input_files=['/Users/aftab/ClassTest/TheHistoryofComputers1.pdf'], file_extractor=file_extractor).aload_data(show_progress=True)\n",
    "    all_text = \"\\n\".join([doc.text_resource.text for doc in documents])\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(load_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_format = \"\"\"\n",
    "{\n",
    "  quiz_title:str,\n",
    "    questions: [\n",
    "      {\n",
    "      \"question_id\": int,\n",
    "      \"question\": str,\n",
    "      \"options\": [{\"option_id\": int, \"option\": str}],\n",
    "      \"correct_option_id\": int\n",
    "      }\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text: str) -> str:\n",
    "    response = summary_chat_session.send_message(f\"Generate datailed and precise summary in points without leaving any small detail from the given content: {text}\")\n",
    "\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_QUESTIONS = 20\n",
    "def generate_quiz(text, num_questions=MAX_QUESTIONS):\n",
    "    response = quiz_chat_session.send_message(f\"Create a quiz of {num_questions} questions returning data in this JSON format: \\n{quiz_format} on the content given below\\n\\n{text}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "answers.append(generate_quiz(all_text,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formatted_json = []\n",
    "for answer in answers:\n",
    "  formatted_json.append(json.loads(answer))\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = generate_summary(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"quiz.json\", \"w\") as f:\n",
    "  json.dump(json.loads(formatted_json), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
